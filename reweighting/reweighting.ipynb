{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reweighting MC simulation to data using a NN\n",
    "Reweighting MC simulations to data is a common task used to improve the modelling. The most common practice is to reweight a single variable of particular importance in an analysis. This example demonstrates how a NN can be used to perform the reweighting task by considering multiple variables together, which improves the modelling across multiple variables and can account for correlations between variables.\n",
    "\n",
    "**Many thanks to Michele Faucci Giannelli, Marilena Bandieramonte and Martina Javurkova**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U tensorflow\n",
    "!pip install -U tensorflow-probability\n",
    "!pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from matplotlib import pyplot as plt\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the variables with models for data and MC that are similar but have differences that we want to reweight. \n",
    "This section generates the data and MC samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_model_var1 = tfd.mixture.Mixture(tfd.Categorical(probs=[0.4, 0.6]),\n",
    "            components=[\n",
    "              tfd.Normal(loc=0., scale=0.1),\n",
    "              tfd.Normal(loc=0., scale=0.8),\n",
    "            ])\n",
    "data_model_var2 = tfd.Exponential(0.2)\n",
    "\n",
    "mc_model_var1 = tfd.mixture.Mixture(tfd.Categorical(probs=[0.3, 0.7]),\n",
    "            components=[\n",
    "              tfd.Normal(loc=0.1, scale=0.1),\n",
    "              tfd.Normal(loc=0., scale=1),\n",
    "            ])\n",
    "mc_model_var2 = tfd.Exponential(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the random data from the model\n",
    "The generated data has 3 columns. The first two are two observed variables, the third is the label: 0 for data, 1 for MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NSAMPLE = 100000\n",
    "data_var1 = data_model_var1.sample(NSAMPLE).numpy()\n",
    "data_var2 = data_model_var2.sample(NSAMPLE).numpy()\n",
    "mc_var1 = mc_model_var1.sample(NSAMPLE).numpy()\n",
    "mc_var2 = mc_model_var2.sample(NSAMPLE).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a rotation to create correlations and make these correlations different in data and MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_var1, data_var2 = 0.7 * data_var1 + 0.3 * data_var2, 0.3 * data_var1 + 0.7 * data_var2\n",
    "\n",
    "mc_var1, mc_var2 = 0.9 * mc_var1 + 0.1 * mc_var2, 0.1 * mc_var1 + 0.9 * mc_var2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the variables in a single vector, put a flag as the last element (0/1 for data/MC respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.vstack([data_var1, data_var2, np.zeros(NSAMPLE)]).T\n",
    "mc = np.vstack([mc_var1, mc_var2, np.ones(NSAMPLE)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply selections to the data\n",
    "Here one can implement selections to focus on specific parts of the distribution. It is especially challenging to model the tails of distributions, so these can be reduced so that the training focuses on the bulk of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_cut1 = data[data[:,0]>-5]\n",
    "#data_cut2 = data_cut1[data_cut1[:,0]<10]\n",
    "#data_cut3 = data_cut2[data_cut2[:,1]<20]\n",
    "#mc_cut1 = mc[mc[:,0]>-5]\n",
    "#mc_cut2 = mc_cut1[mc_cut1[:,0]<10]\n",
    "#mc_cut3 = mc_cut2[mc_cut2[:,1]<20]\n",
    "#data_sel = data_cut3\n",
    "#mc_sel = mc_cut3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create training and testing samples\n",
    "Create a sample by mixing and shuffling data and MC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sample = np.vstack([data_sel, mc_sel])\n",
    "sample = np.vstack([data, mc])\n",
    "np.random.shuffle(sample)\n",
    "NSAMPLE=len(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the dataset into training and testing samples by splitting them in half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_sample = sample[int(NSAMPLE / 2):]\n",
    "testing_sample = sample[:int(NSAMPLE / 2)]\n",
    "\n",
    "X_train = training_sample[:, :-1]\n",
    "Y_train = training_sample[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the variables before the correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_sample_mc = testing_sample[testing_sample[:, -1] == 1]\n",
    "testing_sample_data = testing_sample[testing_sample[:, -1] == 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(13, 6))\n",
    "bins = np.linspace(-2, 4, 100)\n",
    "axs[0].hist(testing_sample_mc[:, 0], bins=bins, histtype='step', label='MC', lw=2)\n",
    "axs[0].hist(testing_sample_data[:, 0], bins=bins, histtype='step', label='data', lw=2)\n",
    "axs[0].legend(loc=0, fontsize=14)\n",
    "axs[0].set_title('var1', fontsize=14)\n",
    "\n",
    "bins = np.linspace(-1, 7, 100)\n",
    "axs[1].hist(testing_sample_mc[:, 1], bins=bins, histtype='step', lw=2)\n",
    "axs[1].hist(testing_sample_data[:, 1], bins=bins, histtype='step', lw=2)\n",
    "axs[1].set_title('var2', fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the classifier, optimizing the cross entropy\n",
    "Optimize a classifier to distinguish between data and MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = tf.keras.Sequential([\n",
    "    tf.keras.Input(X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "classifier.compile(loss='binary_crossentropy', metrics='accuracy')\n",
    "classifier.fit(X_train, Y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the weight on the test sample (the value is close to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight is just the ratio between\n",
    "\n",
    "$$w = \\frac{P[data|q]}{P[MC|q]}$$\n",
    "\n",
    "Where $q$ is our variable(s). Usually this is estimated with a multi-dimensional histogram, or just ignoring correlation using a set of 1D histograms.\n",
    "\n",
    "A perfect classifier will return $y=P[data|x]$. Of course $P[MC|x] = 1 - P[data|x]$, where $x$ are the feature of one event. So:\n",
    "\n",
    "$$w = \\frac{y}{1 - y}$$\n",
    "\n",
    "Note that if we have data very similar to MC we will get a very poor classifier, which will output 0.5, and so the weight will be 1.\n",
    "\n",
    "An additional thing to do (not done here), is to calibrate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(testing_sample_mc))\n",
    "weights = classifier.predict(testing_sample_mc[:, :-1])\n",
    "weights = weights / (1 - weights)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of the weights from the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(weights, bins=np.linspace(0, 1, 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the weights and plot the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(13, 6))\n",
    "bins = np.linspace(-2, 4, 100)\n",
    "axs[0].hist(testing_sample_mc[:, 0], weights=1/weights, bins=bins, histtype='step', label='MC reweighted', lw=2)\n",
    "axs[0].hist(testing_sample_data[:, 0], bins=bins, histtype='step', label='data', lw=2)\n",
    "axs[0].hist(testing_sample_mc[:, 0], bins=bins, histtype='step', label='MC', ls='--')\n",
    "axs[0].legend(loc=0, fontsize=14)\n",
    "axs[0].set_title('var1', fontsize=14)\n",
    "\n",
    "bins = np.linspace(-1, 7, 100)\n",
    "axs[1].hist(testing_sample_mc[:, 1], weights=1/weights, bins=bins, histtype='step', lw=2)\n",
    "axs[1].hist(testing_sample_data[:, 1], bins=bins, histtype='step', lw=2)\n",
    "axs[1].hist(testing_sample_mc[:, 1], bins=bins, histtype='step', label='MC', ls='--')\n",
    "axs[1].set_title('var2', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the correlation\n",
    "Check to see if the reweighed MC reproduces the correlation in data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "bins = [np.linspace(-2, 2, 50), np.linspace(0., 10, 50)]\n",
    "axs[0].hist2d(testing_sample_data[:, 0], testing_sample_data[:, 1], bins=bins)\n",
    "axs[1].hist2d(testing_sample_mc[:, 0], testing_sample_mc[:, 1], bins=bins)\n",
    "axs[2].hist2d(testing_sample_mc[:, 0], testing_sample_mc[:, 1], bins=bins, weights=1/np.squeeze(weights))\n",
    "axs[0].set_title('data', fontsize=14)\n",
    "axs[1].set_title('MC', fontsize=14)\n",
    "axs[2].set_title('MC reweighted', fontsize=14)\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('var1', fontsize=14)\n",
    "    ax.set_ylabel('var2', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare effective statistics with size of the sample\n",
    "We loose a lot of effective statistics. Try running more events to improve the modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = 1/np.squeeze(weights)\n",
    "np.sum(w) ** 2 / np.sum(w ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Try making changes to the configuration of the NN, the input variable data and to see if you can improve the modelling of the bulk of the distribution.\n",
    "1. Implement selections to reduce the tails and reduce biases. \n",
    "2. Try increasing the number of epochs. \n",
    "3. Try changing the activation function on the NN, for example from relu to swish.\n",
    "4. Try changing the structure of the NN. \n",
    "5. Increasing the sample size. Note if you make it too large it will take long to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
